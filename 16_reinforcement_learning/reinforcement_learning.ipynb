{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 16: Reinforcement Learning\n",
    "\n",
    "This tutorial introduces reinforcement learning with PyTorch, covering fundamental algorithms and modern deep RL techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import gym\n",
    "from typing import Tuple, List, Optional\n",
    "import math\n",
    "from IPython import display\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic RL Environment\n",
    "\n",
    "Let's start by creating a simple grid world environment to understand the basics of RL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    \"\"\"Simple grid world environment for RL\"\"\"\n",
    "    def __init__(self, size=5):\n",
    "        self.size = size\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment to initial state\"\"\"\n",
    "        self.agent_pos = [0, 0]\n",
    "        self.goal_pos = [self.size-1, self.size-1]\n",
    "        self.done = False\n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        \"\"\"Get current state representation\"\"\"\n",
    "        state = np.zeros((self.size, self.size))\n",
    "        state[self.agent_pos[0], self.agent_pos[1]] = 1\n",
    "        state[self.goal_pos[0], self.goal_pos[1]] = 2\n",
    "        return state.flatten()\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Take action and return (next_state, reward, done)\"\"\"\n",
    "        if self.done:\n",
    "            return self._get_state(), 0, True\n",
    "        \n",
    "        # Actions: 0=up, 1=right, 2=down, 3=left\n",
    "        moves = [(-1, 0), (0, 1), (1, 0), (0, -1)]\n",
    "        move = moves[action]\n",
    "        \n",
    "        # Update position\n",
    "        new_pos = [self.agent_pos[0] + move[0], self.agent_pos[1] + move[1]]\n",
    "        \n",
    "        # Check boundaries\n",
    "        if 0 <= new_pos[0] < self.size and 0 <= new_pos[1] < self.size:\n",
    "            self.agent_pos = new_pos\n",
    "        \n",
    "        # Check if goal reached\n",
    "        if self.agent_pos == self.goal_pos:\n",
    "            reward = 10\n",
    "            self.done = True\n",
    "        else:\n",
    "            reward = -0.1  # Small negative reward for each step\n",
    "        \n",
    "        return self._get_state(), reward, self.done\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Visualize the environment\"\"\"\n",
    "        grid = np.zeros((self.size, self.size))\n",
    "        grid[self.agent_pos[0], self.agent_pos[1]] = 1\n",
    "        grid[self.goal_pos[0], self.goal_pos[1]] = 2\n",
    "        \n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(grid, cmap='viridis', interpolation='nearest')\n",
    "        plt.colorbar(label='0: Empty, 1: Agent, 2: Goal')\n",
    "        plt.title('Grid World Environment')\n",
    "        plt.xticks(range(self.size))\n",
    "        plt.yticks(range(self.size))\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the environment\n",
    "env = GridWorld(size=5)\n",
    "state = env.reset()\n",
    "print(\"Initial state shape:\", state.shape)\n",
    "print(\"\\nInitial grid:\")\n",
    "env.render()\n",
    "\n",
    "# Take a few random actions\n",
    "print(\"\\nTaking random actions:\")\n",
    "for i in range(5):\n",
    "    action = np.random.randint(4)\n",
    "    next_state, reward, done = env.step(action)\n",
    "    action_names = ['Up', 'Right', 'Down', 'Left']\n",
    "    print(f\"Action: {action_names[action]}, Reward: {reward:.1f}, Done: {done}\")\n",
    "    \n",
    "print(\"\\nFinal grid:\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deep Q-Network (DQN)\n",
    "\n",
    "DQN uses a neural network to approximate Q-values for each action in each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"Deep Q-Network\"\"\"\n",
    "    def __init__(self, state_size, action_size, hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Experience replay buffer\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Experience replay buffer for DQN\"\"\"\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.buffer.append(Transition(*args))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of transitions\"\"\"\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"DQN Agent with experience replay and target network\"\"\"\n",
    "    def __init__(self, state_size, action_size, lr=1e-3, gamma=0.99, \n",
    "                 epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "        # Neural networks\n",
    "        self.q_network = DQN(state_size, action_size).to(device)\n",
    "        self.target_network = DQN(state_size, action_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        \n",
    "        # Experience replay\n",
    "        self.memory = ReplayBuffer()\n",
    "        \n",
    "        # Update target network\n",
    "        self.update_target_network()\n",
    "        \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Copy weights from main network to target network\"\"\"\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "    \n",
    "    def act(self, state, training=True):\n",
    "        \"\"\"Choose action using epsilon-greedy policy\"\"\"\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_size - 1)\n",
    "        \n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_network(state_tensor)\n",
    "        return q_values.argmax().item()\n",
    "    \n",
    "    def remember(self, state, action, next_state, reward, done):\n",
    "        \"\"\"Store transition in replay buffer\"\"\"\n",
    "        self.memory.push(state, action, next_state, reward, done)\n",
    "    \n",
    "    def replay(self, batch_size=32):\n",
    "        \"\"\"Train the network on a batch of transitions\"\"\"\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        \n",
    "        transitions = self.memory.sample(batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        \n",
    "        # Convert to tensors\n",
    "        state_batch = torch.FloatTensor(batch.state).to(device)\n",
    "        action_batch = torch.LongTensor(batch.action).to(device)\n",
    "        reward_batch = torch.FloatTensor(batch.reward).to(device)\n",
    "        next_state_batch = torch.FloatTensor(batch.next_state).to(device)\n",
    "        done_batch = torch.FloatTensor(batch.done).to(device)\n",
    "        \n",
    "        # Current Q values\n",
    "        current_q_values = self.q_network(state_batch).gather(1, action_batch.unsqueeze(1))\n",
    "        \n",
    "        # Next Q values from target network\n",
    "        next_q_values = self.target_network(next_state_batch).max(1)[0].detach()\n",
    "        target_q_values = reward_batch + (self.gamma * next_q_values * (1 - done_batch))\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DQN on GridWorld\n",
    "env = GridWorld(size=5)\n",
    "state_size = env.size * env.size\n",
    "action_size = 4\n",
    "\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "# Training loop\n",
    "episodes = 300\n",
    "scores = []\n",
    "losses = []\n",
    "epsilons = []\n",
    "\n",
    "print(\"Training DQN...\")\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    \n",
    "    while True:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        agent.remember(state, action, next_state, reward, done)\n",
    "        \n",
    "        if len(agent.memory) > 32:\n",
    "            loss = agent.replay()\n",
    "            if loss:\n",
    "                losses.append(loss)\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "        \n",
    "        if done or steps > 100:\n",
    "            break\n",
    "    \n",
    "    scores.append(total_reward)\n",
    "    epsilons.append(agent.epsilon)\n",
    "    \n",
    "    # Update target network\n",
    "    if episode % 10 == 0:\n",
    "        agent.update_target_network()\n",
    "    \n",
    "    if episode % 50 == 0:\n",
    "        avg_score = np.mean(scores[-50:]) if len(scores) >= 50 else np.mean(scores)\n",
    "        print(f\"Episode {episode}, Average Score: {avg_score:.2f}, Epsilon: {agent.epsilon:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize DQN training\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Plot scores\n",
    "axes[0, 0].plot(scores, alpha=0.6)\n",
    "axes[0, 0].plot(np.convolve(scores, np.ones(20)/20, mode='valid'), linewidth=2)\n",
    "axes[0, 0].set_title('DQN Training Scores')\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Score')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot losses\n",
    "if losses:\n",
    "    axes[0, 1].plot(losses, alpha=0.6)\n",
    "    axes[0, 1].set_title('DQN Training Loss')\n",
    "    axes[0, 1].set_xlabel('Training Step')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].set_yscale('log')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot epsilon decay\n",
    "axes[1, 0].plot(epsilons)\n",
    "axes[1, 0].set_title('Epsilon Decay')\n",
    "axes[1, 0].set_xlabel('Episode')\n",
    "axes[1, 0].set_ylabel('Epsilon')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Test the trained agent\n",
    "test_env = GridWorld(size=5)\n",
    "state = test_env.reset()\n",
    "path = [test_env.agent_pos.copy()]\n",
    "\n",
    "for _ in range(20):\n",
    "    action = agent.act(state, training=False)\n",
    "    state, reward, done = test_env.step(action)\n",
    "    path.append(test_env.agent_pos.copy())\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "# Visualize learned path\n",
    "grid = np.zeros((5, 5))\n",
    "for i, pos in enumerate(path):\n",
    "    grid[pos[0], pos[1]] = i + 1\n",
    "grid[4, 4] = len(path) + 1  # Goal\n",
    "\n",
    "im = axes[1, 1].imshow(grid, cmap='viridis')\n",
    "axes[1, 1].set_title('Learned Path')\n",
    "axes[1, 1].set_xlabel('X')\n",
    "axes[1, 1].set_ylabel('Y')\n",
    "plt.colorbar(im, ax=axes[1, 1], label='Step')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Policy Gradient - REINFORCE\n",
    "\n",
    "REINFORCE learns a policy directly by maximizing expected rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"Policy network for REINFORCE\"\"\"\n",
    "    def __init__(self, state_size, action_size, hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x, dim=-1)\n",
    "\n",
    "class REINFORCEAgent:\n",
    "    \"\"\"REINFORCE policy gradient agent\"\"\"\n",
    "    def __init__(self, state_size, action_size, lr=1e-3, gamma=0.99):\n",
    "        self.gamma = gamma\n",
    "        self.policy = PolicyNetwork(state_size, action_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        \n",
    "    def act(self, state):\n",
    "        \"\"\"Select action from policy\"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        probs = self.policy(state_tensor)\n",
    "        m = torch.distributions.Categorical(probs)\n",
    "        action = m.sample()\n",
    "        self.saved_log_probs.append(m.log_prob(action))\n",
    "        return action.item()\n",
    "    \n",
    "    def store_reward(self, reward):\n",
    "        \"\"\"Store reward for current episode\"\"\"\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Update policy using collected rewards\"\"\"\n",
    "        R = 0\n",
    "        policy_loss = []\n",
    "        returns = []\n",
    "        \n",
    "        # Calculate discounted returns\n",
    "        for r in self.rewards[::-1]:\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        \n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
    "        \n",
    "        # Calculate loss\n",
    "        for log_prob, R in zip(self.saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "        \n",
    "        # Update policy\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Clear episode data\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        \n",
    "        return policy_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train REINFORCE\n",
    "env = GridWorld(size=5)\n",
    "reinforce_agent = REINFORCEAgent(state_size, action_size)\n",
    "scores = []\n",
    "losses = []\n",
    "\n",
    "print(\"Training REINFORCE...\")\n",
    "for episode in range(300):\n",
    "    state = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        action = reinforce_agent.act(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        reinforce_agent.store_reward(reward)\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    loss = reinforce_agent.train()\n",
    "    losses.append(loss)\n",
    "    scores.append(sum(reinforce_agent.rewards))\n",
    "    \n",
    "    if episode % 50 == 0:\n",
    "        avg_score = np.mean(scores[-50:]) if len(scores) >= 50 else np.mean(scores)\n",
    "        print(f\"Episode {episode}, Average Score: {avg_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Actor-Critic (A2C)\n",
    "\n",
    "Actor-Critic methods combine the benefits of value-based and policy-based approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    \"\"\"Combined Actor-Critic network\"\"\"\n",
    "    def __init__(self, state_size, action_size, hidden_size=128):\n",
    "        super().__init__()\n",
    "        # Shared layers\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Actor head\n",
    "        self.actor = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "        # Critic head\n",
    "        self.critic = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        # Actor output (action probabilities)\n",
    "        action_probs = F.softmax(self.actor(x), dim=-1)\n",
    "        \n",
    "        # Critic output (state value)\n",
    "        state_value = self.critic(x)\n",
    "        \n",
    "        return action_probs, state_value\n",
    "\n",
    "class A2CAgent:\n",
    "    \"\"\"Advantage Actor-Critic agent\"\"\"\n",
    "    def __init__(self, state_size, action_size, lr=1e-3, gamma=0.99):\n",
    "        self.gamma = gamma\n",
    "        self.actor_critic = ActorCritic(state_size, action_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=lr)\n",
    "        \n",
    "    def act(self, state):\n",
    "        \"\"\"Select action and return value estimate\"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action_probs, _ = self.actor_critic(state_tensor)\n",
    "        \n",
    "        m = torch.distributions.Categorical(action_probs)\n",
    "        action = m.sample()\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def train_step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Single step training update\"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Get current predictions\n",
    "        action_probs, value = self.actor_critic(state_tensor)\n",
    "        \n",
    "        # Get next state value\n",
    "        with torch.no_grad():\n",
    "            _, next_value = self.actor_critic(next_state_tensor)\n",
    "            target_value = reward + self.gamma * next_value * (1 - done)\n",
    "        \n",
    "        # Calculate advantage\n",
    "        advantage = target_value - value\n",
    "        \n",
    "        # Actor loss\n",
    "        m = torch.distributions.Categorical(action_probs)\n",
    "        actor_loss = -m.log_prob(torch.tensor(action).to(device)) * advantage.detach()\n",
    "        \n",
    "        # Critic loss\n",
    "        critic_loss = F.mse_loss(value, target_value.detach())\n",
    "        \n",
    "        # Total loss\n",
    "        loss = actor_loss + critic_loss\n",
    "        \n",
    "        # Update\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train A2C\n",
    "env = GridWorld(size=5)\n",
    "a2c_agent = A2CAgent(state_size, action_size)\n",
    "scores = []\n",
    "losses = []\n",
    "\n",
    "print(\"Training A2C...\")\n",
    "for episode in range(300):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    episode_losses = []\n",
    "    \n",
    "    while True:\n",
    "        action = a2c_agent.act(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        loss = a2c_agent.train_step(state, action, reward, next_state, done)\n",
    "        episode_losses.append(loss)\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    scores.append(total_reward)\n",
    "    losses.extend(episode_losses)\n",
    "    \n",
    "    if episode % 50 == 0:\n",
    "        avg_score = np.mean(scores[-50:]) if len(scores) >= 50 else np.mean(scores)\n",
    "        print(f\"Episode {episode}, Average Score: {avg_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison of Algorithms\n",
    "\n",
    "Let's compare the performance of different RL algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all algorithms and compare\n",
    "algorithms = ['DQN', 'REINFORCE', 'A2C']\n",
    "all_scores = {}\n",
    "\n",
    "# Train each algorithm\n",
    "for algo in algorithms:\n",
    "    env = GridWorld(size=5)\n",
    "    scores = []\n",
    "    \n",
    "    if algo == 'DQN':\n",
    "        agent = DQNAgent(state_size, action_size)\n",
    "    elif algo == 'REINFORCE':\n",
    "        agent = REINFORCEAgent(state_size, action_size)\n",
    "    else:  # A2C\n",
    "        agent = A2CAgent(state_size, action_size)\n",
    "    \n",
    "    print(f\"\\nTraining {algo}...\")\n",
    "    for episode in range(200):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        if algo == 'REINFORCE':\n",
    "            while True:\n",
    "                action = agent.act(state)\n",
    "                next_state, reward, done = env.step(action)\n",
    "                agent.store_reward(reward)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                if done:\n",
    "                    break\n",
    "            agent.train()\n",
    "        elif algo == 'DQN':\n",
    "            while True:\n",
    "                action = agent.act(state)\n",
    "                next_state, reward, done = env.step(action)\n",
    "                agent.remember(state, action, next_state, reward, done)\n",
    "                if len(agent.memory) > 32:\n",
    "                    agent.replay()\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                if done:\n",
    "                    break\n",
    "        else:  # A2C\n",
    "            while True:\n",
    "                action = agent.act(state)\n",
    "                next_state, reward, done = env.step(action)\n",
    "                agent.train_step(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                if done:\n",
    "                    break\n",
    "        \n",
    "        scores.append(total_reward)\n",
    "    \n",
    "    all_scores[algo] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for algo, scores in all_scores.items():\n",
    "    smoothed = np.convolve(scores, np.ones(20)/20, mode='valid')\n",
    "    plt.plot(smoothed, label=algo, linewidth=2)\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Score (20 episode window)')\n",
    "plt.title('RL Algorithm Comparison on GridWorld')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Print final statistics\n",
    "print(\"\\nFinal Performance (last 50 episodes):\")\n",
    "for algo, scores in all_scores.items():\n",
    "    final_avg = np.mean(scores[-50:])\n",
    "    final_std = np.std(scores[-50:])\n",
    "    print(f\"{algo}: {final_avg:.2f} Â± {final_std:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Concepts: Continuous Control\n",
    "\n",
    "For continuous action spaces, we need different approaches like DDPG or PPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous control environment simulation\n",
    "class ContinuousControlEnv:\n",
    "    \"\"\"Simple continuous control environment\"\"\"\n",
    "    def __init__(self):\n",
    "        self.state = None\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = np.random.uniform(-1, 1, size=2)\n",
    "        return self.state.copy()\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Apply action (continuous)\n",
    "        self.state += action * 0.1\n",
    "        \n",
    "        # Reward is negative distance from origin\n",
    "        reward = -np.linalg.norm(self.state)\n",
    "        \n",
    "        # Episode ends if too far from origin\n",
    "        done = np.linalg.norm(self.state) > 2.0\n",
    "        \n",
    "        return self.state.copy(), reward, done\n",
    "\n",
    "# DDPG Networks\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor network for continuous actions\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, action_dim)\n",
    "        self.max_action = max_action\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        return x * self.max_action\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic network for Q-value estimation\"\"\"\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize continuous control task\n",
    "env = ContinuousControlEnv()\n",
    "states = []\n",
    "actions = []\n",
    "\n",
    "# Random policy\n",
    "state = env.reset()\n",
    "for _ in range(50):\n",
    "    action = np.random.uniform(-1, 1, size=2)\n",
    "    states.append(state.copy())\n",
    "    actions.append(action)\n",
    "    state, reward, done = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "states = np.array(states)\n",
    "\n",
    "# Plot trajectory\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(states[:, 0], states[:, 1], 'b-', alpha=0.6, label='Trajectory')\n",
    "plt.scatter(states[0, 0], states[0, 1], c='green', s=100, marker='o', label='Start')\n",
    "plt.scatter(states[-1, 0], states[-1, 1], c='red', s=100, marker='x', label='End')\n",
    "plt.scatter(0, 0, c='gold', s=200, marker='*', label='Goal')\n",
    "plt.xlim(-2.5, 2.5)\n",
    "plt.ylim(-2.5, 2.5)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Continuous Control Task')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "Let's summarize the key concepts and provide practical guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Algorithm comparison\n",
    "algorithms = ['DQN', 'REINFORCE', 'A2C', 'DDPG', 'PPO']\n",
    "properties = ['Sample\\nEfficiency', 'Stability', 'Continuous\\nActions', 'Discrete\\nActions']\n",
    "scores = np.array([\n",
    "    [4, 3, 1, 5],  # DQN\n",
    "    [2, 2, 3, 4],  # REINFORCE\n",
    "    [3, 4, 3, 4],  # A2C\n",
    "    [4, 3, 5, 1],  # DDPG\n",
    "    [4, 5, 5, 5],  # PPO\n",
    "])\n",
    "\n",
    "im = axes[0, 0].imshow(scores, cmap='YlOrRd', aspect='auto')\n",
    "axes[0, 0].set_xticks(range(len(properties)))\n",
    "axes[0, 0].set_xticklabels(properties)\n",
    "axes[0, 0].set_yticks(range(len(algorithms)))\n",
    "axes[0, 0].set_yticklabels(algorithms)\n",
    "axes[0, 0].set_title('Algorithm Properties (1-5 scale)')\n",
    "plt.colorbar(im, ax=axes[0, 0])\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(algorithms)):\n",
    "    for j in range(len(properties)):\n",
    "        axes[0, 0].text(j, i, str(scores[i, j]), ha='center', va='center')\n",
    "\n",
    "# Action space comparison\n",
    "action_types = ['Discrete', 'Continuous']\n",
    "algo_support = {\n",
    "    'DQN': [1, 0],\n",
    "    'REINFORCE': [1, 0.7],\n",
    "    'A2C': [1, 0.7],\n",
    "    'DDPG': [0, 1],\n",
    "    'PPO': [1, 1]\n",
    "}\n",
    "\n",
    "x = np.arange(len(algorithms))\n",
    "width = 0.35\n",
    "\n",
    "for i, action_type in enumerate(action_types):\n",
    "    values = [algo_support[algo][i] for algo in algorithms]\n",
    "    axes[0, 1].bar(x + i*width - width/2, values, width, label=action_type)\n",
    "\n",
    "axes[0, 1].set_xlabel('Algorithm')\n",
    "axes[0, 1].set_ylabel('Support Level')\n",
    "axes[0, 1].set_title('Action Space Support')\n",
    "axes[0, 1].set_xticks(x)\n",
    "axes[0, 1].set_xticklabels(algorithms)\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Sample complexity\n",
    "sample_complexity = {\n",
    "    'DQN': 1000,\n",
    "    'REINFORCE': 5000,\n",
    "    'A2C': 2000,\n",
    "    'DDPG': 1500,\n",
    "    'PPO': 1200\n",
    "}\n",
    "\n",
    "axes[1, 0].bar(algorithms, sample_complexity.values(), color='skyblue')\n",
    "axes[1, 0].set_ylabel('Episodes to Convergence')\n",
    "axes[1, 0].set_title('Sample Complexity (Lower is Better)')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Best practices text\n",
    "axes[1, 1].text(0.1, 0.9, 'RL Best Practices:', fontsize=14, fontweight='bold', transform=axes[1, 1].transAxes)\n",
    "practices = [\n",
    "    '1. Start simple: Use DQN for discrete, DDPG for continuous',\n",
    "    '2. Normalize observations and rewards',\n",
    "    '3. Use experience replay for off-policy methods',\n",
    "    '4. Implement proper exploration strategies',\n",
    "    '5. Monitor multiple metrics during training',\n",
    "    '6. Use stable baselines for production',\n",
    "    '7. Consider PPO for robust performance',\n",
    "    '8. Debug with simple environments first'\n",
    "]\n",
    "\n",
    "for i, practice in enumerate(practices):\n",
    "    axes[1, 1].text(0.1, 0.8 - i*0.1, practice, fontsize=11, transform=axes[1, 1].transAxes)\n",
    "\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we covered:\n",
    "\n",
    "1. **Deep Q-Networks (DQN)**: Value-based method for discrete actions\n",
    "2. **REINFORCE**: Simple policy gradient method\n",
    "3. **Actor-Critic (A2C)**: Combining value and policy methods\n",
    "4. **Continuous Control**: DDPG for continuous action spaces\n",
    "5. **Best Practices**: Practical tips for successful RL\n",
    "\n",
    "### Key Takeaways:\n",
    "- RL is about learning from interaction\n",
    "- Different algorithms suit different problems\n",
    "- Exploration vs exploitation is crucial\n",
    "- Start simple and scale up gradually\n",
    "- Modern methods like PPO offer good stability\n",
    "\n",
    "Reinforcement learning is a powerful paradigm for solving sequential decision-making problems!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}